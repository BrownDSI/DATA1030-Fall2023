{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **After going through the two lectures on feature importance, I'm curious about its application in large-scale datasets. Should we employ a global feature importance model to identify and select only the key features for use in the machine learning model, as a means to reduce computational demand? Or is it more appropriate to apply global feature importance analysis only after the machine learning algorithm has been executed? Especially for the large-scale dataset that has 100+ features.**\n",
    "    - As long as you can work with your dataset on your laptop, it is not large scale, so a dataset with ~100 features is definitely not large by today's computational standards.\n",
    "    - Global feature importances can only be calculate **after** you train a machine learning algorithm\n",
    "    - Once you train an ML modl using all features and calculate the global importances, you could decide to drop certain features and re-train the model with a smaller number of features\n",
    "- **In this lecture, we focused on using the SHAP method and visualizing the results of classification problems, does this method work for regression problems? And can the same sort of visualizations be used?**\n",
    "    - Yep, it works similarly in regression models too\n",
    "    - The only difference is that the SHAP array will only have two dimensions (n_points, n_ftrs)\n",
    "    - We had a third dimension in classficiation, the number of classes but that's not necessary in regression because we predict one continuos target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on imbalanced datasets\n",
    "- we learnt that a classification problem is imbalanced if more than 90-95% of the points belong to one class (class 0) and only a small fraction of the points belong to the other class (class 1)\n",
    "    - fraud detection\n",
    "    - sick or not sick (usually by far most people are not sick)\n",
    "- we learnt to not use a metric that relies on the True Negatives in the confusion matrix\n",
    "    - no accuracy or ROC\n",
    "    - use f_beta or the precision-recall curve instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else can I do if I have an imbalanced dataset?\n",
    "- most (but not all) classification algorithms we covered have a parameter called `class_weight` which allows you to assign more weight to the class 1 point\n",
    "    - a misclassified class 1 point will contribute more to the cost function than a misclassified class 0 point\n",
    "    - read the manual on `class_weight` because the different algorithms have slightly different definitions for this parameter\n",
    "    - usually you can use `None`, `balanced`, or manually define what the class weight should be\n",
    "    - it is worthwhile to tune this parameter if you have an imbalanced dataset\n",
    "- resample/augment the dataset\n",
    "    - SMOTE (Synthetic Minority Over-sampling Technique), see the [paper](https://arxiv.org/abs/1106.1813)\n",
    "    - to improve the balance of the problem, new class 1 examples are synthesized from the existing examples\n",
    "    - be careful though!\n",
    "        - while resampling improves the balance of the dataset, the results of the model can be misleading\n",
    "        - when you deploy the model, the incoming data will be as imbalanced as the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misleading results with resampling\n",
    "- let's assume you have an imbalanced dataset with 99% of points in class 0 and 1% of points in class 1\n",
    "- you resample it such that the improved class balance is 50-50\n",
    "- here the confusion matrix of the trained model:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0)</td>\n",
    "        <td><b>True Negative (TN): 45%</b></td>\n",
    "        <td><b>False Positive (FP): 5%</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1)</td>\n",
    "        <td><b>False Negative (FN): 5%</b></td>\n",
    "        <td><b>True Positive (TP): 45%</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- 90% accuracy which is well above the 50% baseline accuracy!\n",
    "- the precision, recall, and f1 scores are all 0.9.\n",
    "- it looks great, doesn't it?\n",
    "- let's rewrite the confusion matrix to reflect rates with respect to the Condition Negative and Condition Positive points!\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0) 50% of the points</td>\n",
    "        <td><b>90% of CNs are correctly classified</b></td>\n",
    "        <td><b>10% of CNs are incorrectly classified</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1) 50% of the points</td>\n",
    "        <td><b>10% of CPs are incorrectly classified</b></td>\n",
    "        <td><b>90% of CPs are correctly classified</b></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's deploy this model\n",
    "\n",
    "- the incoming data has the same balance as the original dataset (99% to 1%)\n",
    "- let's assume we have 1e5 new points, 9.9e4 belongs to class 0, 1000 belongs to class 1\n",
    "- what will be the numbers in the confusion matrix?\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0) 99000 points</td>\n",
    "        <td><b>True Negative (TN): 99000 * 0.9 = 89100 </b></td>\n",
    "        <td><b>False Positive (FP): 99000 * 0.1 = 9900 </b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1) 1000 points</td>\n",
    "        <td><b>False Negative (FN): 1000 * 0.1 = 100</b></td>\n",
    "        <td><b>True Positive (TP): 1000 * 0.9 = 900</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- the accuracy of this model is still 0.90 but now it is well below the baseline of 0.99!\n",
    "- recall is good (0.90) but the precision is not great (~0.083)\n",
    "- the f1 score is ~0.15\n",
    "- the false positives are overwhelming\n",
    "- this is why you need to be careful with resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
